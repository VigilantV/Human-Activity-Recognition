{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Action Recognition — EfficientNetB3\n",
    "\n",
    "Train an ImageNet-pretrained EfficientNetB3 to classify human actions from images.\n",
    "\n",
    "**What’s inside**\n",
    "- Fast `tf.data` pipeline (decode/resize/augment) + optional MixUp\n",
    "- Mixed precision for faster training\n",
    "- Two-stage training: warm up the head → fine-tune the backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from google.protobuf.message_factory import MessageFactory\n",
    "\n",
    "if not hasattr(MessageFactory, \"GetPrototype\") and hasattr(MessageFactory, \"GetMessageClass\"):\n",
    "\n",
    "    def _GetPrototype(self, descriptor):\n",
    "        return self.GetMessageClass(descriptor)\n",
    "\n",
    "    MessageFactory.GetPrototype = _GetPrototype\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load the labels CSV, verify that each referenced image exists, then create a stratified 80/20 train/validation split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/human-action-recognition-har-dataset/Human Action Recognition\"\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TRAIN_CSV_PATH = os.path.join(DATA_DIR, \"Training_set.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(csv_path: str, img_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV (filename,label) and add an image filepath column.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    required = {\"filename\", \"label\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(f\"Expected columns {required}, got: {list(df.columns)}\")\n",
    "\n",
    "    df = df[[\"filename\", \"label\"]].copy()\n",
    "    df[\"filepath\"] = df[\"filename\"].map(lambda x: os.path.join(img_dir, x))\n",
    "\n",
    "    missing = (~df[\"filepath\"].map(os.path.exists)).sum()\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"{missing} image files listed in CSV were not found under: {img_dir}\")\n",
    "\n",
    "    return df[[\"filepath\", \"label\"]]\n",
    "\n",
    "\n",
    "def train_val_split(df: pd.DataFrame, seed: int = SEED, train_size: float = 0.8):\n",
    "    \"\"\"Stratified train/val split.\"\"\"\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        train_size=train_size,\n",
    "        shuffle=True,\n",
    "        random_state=seed,\n",
    "        stratify=df[\"label\"],\n",
    "    )\n",
    "    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataframe(TRAIN_CSV_PATH, TRAIN_IMG_DIR)\n",
    "train_df, val_df = train_val_split(df)\n",
    "\n",
    "CLASS_NAMES = sorted(df[\"label\"].unique().tolist())\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"rows={len(df)} train={len(train_df)} val={len(val_df)} classes={NUM_CLASSES}\")\n",
    "assert set(train_df[\"filepath\"]).isdisjoint(set(val_df[\"filepath\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick look (9 samples)\n",
    "\n",
    "Quick sanity check: sample 9 images from the dataframe and plot them in a 3×3 grid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples_grid(frame: pd.DataFrame, n: int = 9, seed: int | None = None):\n",
    "    \"\"\"Plot a 3x3 grid of random samples from a dataframe with filepath/label columns.\"\"\"\n",
    "\n",
    "    n = int(n)\n",
    "    n = 9 if n <= 0 else n\n",
    "    n = min(n, 9)\n",
    "\n",
    "    if seed is None:\n",
    "        import secrets\n",
    "\n",
    "        seed = secrets.randbits(32)\n",
    "\n",
    "    sample = frame.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    cols = 3\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax in axes[n:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i in range(n):\n",
    "        path = sample.loc[i, \"filepath\"]\n",
    "        label = sample.loc[i, \"label\"]\n",
    "\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "        axes[i].imshow(img.numpy())\n",
    "        axes[i].set_title(str(label))\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples_grid(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "Decode + resize images, apply lightweight augmentation, then build efficient `tf.data` datasets for training/validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = \"./\"\n",
    "IMAGE_SIZE = (300, 300)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "PREFETCH = tf.data.AUTOTUNE\n",
    "\n",
    "STEPS_PER_EXECUTION = 64\n",
    "\n",
    "LABEL_LOOKUP = keras.layers.StringLookup(\n",
    "    vocabulary=CLASS_NAMES,\n",
    "    mask_token=None,\n",
    "    num_oov_indices=0,\n",
    "    dtype=tf.int32,\n",
    ")\n",
    "\n",
    "AUGMENTATION = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.08),\n",
    "        keras.layers.RandomZoom(0.15),\n",
    "        keras.layers.RandomTranslation(0.1, 0.1),\n",
    "        keras.layers.RandomContrast(0.1),\n",
    "    ],\n",
    "    name=\"augmentation\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_resize(path: tf.Tensor, label: tf.Tensor):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3, dct_method=\"INTEGER_FAST\")\n",
    "\n",
    "    image = tf.image.resize_with_pad(\n",
    "        image,\n",
    "        IMAGE_SIZE[0],\n",
    "        IMAGE_SIZE[1],\n",
    "        method=\"bilinear\",\n",
    "        antialias=False,\n",
    "    )\n",
    "\n",
    "    image = tf.cast(image, tf.float16)\n",
    "    return image, LABEL_LOOKUP(label)\n",
    "\n",
    "\n",
    "def to_one_hot(images, labels_int):\n",
    "    return images, tf.one_hot(labels_int, depth=NUM_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MixUp\n",
    "\n",
    "Optional regularization: MixUp blends pairs of images/labels on each batch after one-hot encoding (probability `MIXUP_PROB`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXUP_PROB = 0.20\n",
    "MIXUP_ALPHA = 0.2\n",
    "\n",
    "\n",
    "def _sample_beta(batch_size: tf.Tensor, a: float):\n",
    "    a = tf.cast(a, tf.float32)\n",
    "    g1 = tf.random.gamma([batch_size], a)\n",
    "    g2 = tf.random.gamma([batch_size], a)\n",
    "    return g1 / (g1 + g2)\n",
    "\n",
    "\n",
    "def mixup_batch(images, labels):\n",
    "    b = tf.shape(images)[0]\n",
    "    lam = _sample_beta(b, MIXUP_ALPHA)\n",
    "\n",
    "    lam_x = tf.cast(tf.reshape(lam, [b, 1, 1, 1]), images.dtype)\n",
    "    lam_y = tf.cast(tf.reshape(lam, [b, 1]), labels.dtype)\n",
    "\n",
    "    idx = tf.random.shuffle(tf.range(b))\n",
    "    images2 = tf.gather(images, idx)\n",
    "    labels2 = tf.gather(labels, idx)\n",
    "\n",
    "    mixed_images = images * lam_x + images2 * (1.0 - lam_x)\n",
    "    mixed_labels = labels * lam_y + labels2 * (1.0 - lam_y)\n",
    "    return mixed_images, mixed_labels\n",
    "\n",
    "\n",
    "def maybe_mixup(images, labels):\n",
    "    return tf.cond(\n",
    "        tf.random.uniform([]) < MIXUP_PROB,\n",
    "        lambda: mixup_batch(images, labels),\n",
    "        lambda: (images, labels),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 8192\n",
    "\n",
    "# Use disk cache to avoid large RAM usage.\n",
    "VAL_CACHE_PATH = os.path.join(WORKING_DIR, \"val_cache\")\n",
    "\n",
    "\n",
    "def make_dataset(frame: pd.DataFrame, training: bool, apply_mixup: bool = True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((frame[\"filepath\"].values, frame[\"label\"].values))\n",
    "\n",
    "    if training:\n",
    "        ds = ds.shuffle(min(len(frame), SHUFFLE_BUFFER), seed=SEED, reshuffle_each_iteration=True)\n",
    "\n",
    "    ds = ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if not training:\n",
    "        ds = ds.cache(VAL_CACHE_PATH)\n",
    "\n",
    "    # Let TF pick best performance kernels (slightly non-deterministic)\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_deterministic = False\n",
    "    ds = ds.with_options(options)\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=training)\n",
    "    ds = ds.map(to_one_hot, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if training and apply_mixup:\n",
    "        ds = ds.map(maybe_mixup, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return ds.prefetch(PREFETCH)\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_df, training=True, apply_mixup=True)\n",
    "val_ds = make_dataset(val_df, training=False)\n",
    "\n",
    "print(\n",
    "    f\"train_batches={tf.data.experimental.cardinality(train_ds).numpy()} \"\n",
    "    f\"val_batches={tf.data.experimental.cardinality(val_ds).numpy()}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "EfficientNetB3 (ImageNet pretrained) with a lightweight classification head for your action labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(image_size, num_classes: int, head_dropout: float = 0.3):\n",
    "    inputs = keras.Input(shape=(*image_size, 3))\n",
    "\n",
    "    backbone = keras.applications.efficientnet.EfficientNetB3(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(*image_size, 3),\n",
    "    )\n",
    "    backbone.trainable = False\n",
    "\n",
    "    x = AUGMENTATION(inputs)\n",
    "    x = backbone(x, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(head_dropout)(x)\n",
    "\n",
    "    # Keep softmax output in float32 for numerical stability under mixed precision\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"EfficientNetB3_classifier\")\n",
    "    return model, backbone\n",
    "\n",
    "\n",
    "def compile_model(model: keras.Model, lr, weight_decay: float = 1e-4):\n",
    "    AdamW = getattr(keras.optimizers, \"AdamW\", None) or keras.optimizers.experimental.AdamW\n",
    "    model.compile(\n",
    "        optimizer=AdamW(learning_rate=lr, weight_decay=weight_decay),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "        steps_per_execution=STEPS_PER_EXECUTION,\n",
    "    )\n",
    "\n",
    "\n",
    "model, backbone = build_model(IMAGE_SIZE, NUM_CLASSES)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Two-stage training:\n",
    "\n",
    "- Warmup: train only the classification head (backbone frozen)\n",
    "- Fine-tune: unfreeze the last N backbone layers and continue training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_EPOCHS = 2\n",
    "FINETUNE_EPOCHS = 13\n",
    "TOTAL_EPOCHS = WARMUP_EPOCHS + FINETUNE_EPOCHS\n",
    "\n",
    "FINE_TUNE_LAST_N = 160\n",
    "\n",
    "LR_HEAD = 5e-4\n",
    "LR_FINE = 5e-5\n",
    "\n",
    "WEIGHT_DECAY_HEAD = 1e-4\n",
    "WEIGHT_DECAY_FINE = 5e-5\n",
    "\n",
    "\n",
    "def _validate_training_config():\n",
    "    assert isinstance(WARMUP_EPOCHS, int) and WARMUP_EPOCHS >= 1\n",
    "    assert isinstance(FINETUNE_EPOCHS, int) and FINETUNE_EPOCHS >= 1\n",
    "    assert TOTAL_EPOCHS == WARMUP_EPOCHS + FINETUNE_EPOCHS\n",
    "    assert isinstance(FINE_TUNE_LAST_N, int) and FINE_TUNE_LAST_N > 0\n",
    "    assert 0.0 < LR_HEAD and 0.0 < LR_FINE\n",
    "    assert 0.0 <= WEIGHT_DECAY_HEAD and 0.0 <= WEIGHT_DECAY_FINE\n",
    "\n",
    "\n",
    "_validate_training_config()\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(WORKING_DIR, \"best_effnetb3.weights.h5\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    CHECKPOINT_PATH,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "callbacks_warmup = [checkpoint_cb]\n",
    "\n",
    "steps_per_epoch = int(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "SCHED_FINE = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=LR_FINE,\n",
    "    decay_steps=steps_per_epoch * FINETUNE_EPOCHS,\n",
    "    alpha=0.1,\n",
    ")\n",
    "\n",
    "callbacks_finetune = [\n",
    "    checkpoint_cb,\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=6,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    keras.callbacks.TerminateOnNaN(),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 1: warmup (train classification head) ---\n",
    "backbone.trainable = False\n",
    "compile_model(model, lr=LR_HEAD, weight_decay=WEIGHT_DECAY_HEAD)\n",
    "\n",
    "history_warmup = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=WARMUP_EPOCHS,\n",
    "    callbacks=callbacks_warmup,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 2: unfreeze backbone and fine-tune ---\n",
    "\n",
    "def set_finetune_trainable(last_n: int):\n",
    "    \"\"\"Unfreeze last `last_n` layers; keep BatchNorm frozen for stability.\"\"\"\n",
    "\n",
    "    backbone.trainable = True\n",
    "\n",
    "    for layer in backbone.layers[:-last_n]:\n",
    "        layer.trainable = False\n",
    "    for layer in backbone.layers[-last_n:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    for layer in backbone.layers:\n",
    "        if isinstance(layer, keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "\n",
    "set_finetune_trainable(FINE_TUNE_LAST_N)\n",
    "compile_model(model, lr=SCHED_FINE, weight_decay=WEIGHT_DECAY_FINE)\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=WARMUP_EPOCHS,\n",
    "    epochs=TOTAL_EPOCHS,\n",
    "    callbacks=callbacks_finetune,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize: load best checkpoint and merge history\n",
    "\n",
    "model.load_weights(CHECKPOINT_PATH)\n",
    "\n",
    "history = {}\n",
    "for h in (history_warmup, history_finetune):\n",
    "    for k, v in h.history.items():\n",
    "        history.setdefault(k, []).extend(v)\n",
    "\n",
    "best_val_acc = float(np.max(history.get(\"val_accuracy\", [float(\"nan\")])))\n",
    "print(\"best val_accuracy:\", best_val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Plot learning curves and export the best model weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_plot(history_dict):\n",
    "    tacc = history_dict.get(\"accuracy\", [])\n",
    "    tloss = history_dict.get(\"loss\", [])\n",
    "    vacc = history_dict.get(\"val_accuracy\", [])\n",
    "    vloss = history_dict.get(\"val_loss\", [])\n",
    "\n",
    "    epochs = range(1, len(tloss) + 1)\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    axes[0].plot(epochs, tloss, label=\"train\")\n",
    "    axes[0].plot(epochs, vloss, label=\"val\")\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs, tacc, label=\"train\")\n",
    "    axes[1].plot(epochs, vacc, label=\"val\")\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tr_plot(history)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2232355,
     "sourceId": 3733921,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
